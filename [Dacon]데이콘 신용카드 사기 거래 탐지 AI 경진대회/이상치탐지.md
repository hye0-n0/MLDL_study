# Anomaly Detection

**정상 / 비정상(이상치, 특이치) 샘플을 구별하는 알고리즘**

## 적용 분야

- **Fraud Detection System (이상 거래 탐지)**: 신용, 카드, 보험과 같은 금융계에서 불법 및 악용 사례를 탐지
- **Intrusion Detection System (침입 탐지 시스템)**: 컴퓨터 또는 네트워크 시스템에 대한 원치 않는 조작을 탐지
- **Predictive Maintenance (설비 예지 보전)**: 설비가 고장나기 전, 이상 신호를 탐지
- **Customer Churn (고객 이탈)**: 제품 또는 서비스에서 구매와 사용을 중지하는 고객을 분석하는 분야

---

## 비정상 샘플 정의 방식에 따른 분류

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6b1d67bd-187b-46d1-a3a7-02e041a0f1fb/Untitled.png)

- Novelty Detection : 이전에 없던 형태의 새로운 정상 샘플 찾기
- Outlier Detection : 정상 샘플과 전혀 관련 없는 샘플 찾기

---

## 종류

- Supervised Anomaly Detection
    - 학습 시, 라벨링된 정상/비정상 데이터 모두 사용
    - 데이터 불균형 문제 해결
- Semi-supervised Anomaly Detection
    - 정상 데이터만 사용해 학습한 후,  비정상 데이터를 테스트로 입력되었을 때 정상 데이터 특징에 부합하지 않아 이상치로 탐지
    - 학습 시, 라벨링된 정상 데이터만 사용
- Unsupervised Anomaly Detection
    - 데이터에 라벨링이 없을 경우
    - 모델 학습 후, 정상/비정상 구분에 대한 임계치 설정 필요

---

## 방법론

1. **모델 기반**
    - Isolation Forest : Tree based method로 데이터를 분할 및 고립시켜 이상치 탐지
    - 1-class SVM : 데이터가 존재하는 영역을 정의해, 영역 밖의 데이터를 이상치로 간주
2. **거리/밀도 기반**
    - Gaussian Mixture Model : 데이터의 각 샘플의 분포를 파악한 후, 밀도가 임계치보다 낮은 지역에 있는 샘플을 이상치로 간주
    - KNN : k개의 근접 이웃까지 거리를 계산해 데이터를 군집한 후, 이상치 탐지
    - LOF(Local Outlier Factors) : 데이터의 밀도/거리 척도를 통해, 다수 군집과 소수 군집을 생성해 이상치 탐지
3. **재구축**
    - PCA(Principal Component Analysis) : 데이터 사영/복원하여 복원된 정도로 이상치 판단
    - Auto-Encoder : 데이터 압축/복원하여 복원된 정도로 이상치 판단
    
    [Auto Encoder(오토 인코더)](https://www.notion.so/Auto-Encoder-825ecdfa16cb42a1901c64cd7cbb8beb)
    

---

# LOF(Local Outlier Factors)

Density-based Methods

데이터의 거리와 밀도까지 상대적으로 고려하는 모델

![https://i.imgur.com/u5Jy8zs.png](https://i.imgur.com/u5Jy8zs.png)

거리만 고려한다면 O1만 이상치로 판단하고, O2는 정상데이터로 판단. 하지만 C2의 밀집된 데이터를 고려한다면 밀집한 곳 밖에 존재하기 때문에 O2도 이상치라 판단 즉, 밀집 지역에서 밀도의 급격한 감소가 나타남

## k-distance

$k-distance(A)$ : A로부터 k번째 근접 이웃까지의 거리

$**N_k(A)$ :** $k-distance(A)$ 안에 들어오는 이웃의 객체 집합

→ 즉, $k-distance(A)$ 보다 작거나 같은 거리를 가지는 수

![https://i.imgur.com/SRJyXZv.png](https://i.imgur.com/SRJyXZv.png)

## reachability distance

$reachability - distance_k(A, B) = max(k-distance(B), dist(A, B))$

: A에서 B까지의 거리와 B의 k-distance 값 중 큰 값

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/135f5747-5e66-44d6-99dc-d4887d7c772c/Untitled.png)

⇒ 자기자신A와 가까운 이웃과의 RD는 k-distance(A), 먼 이웃은 distance(실제거리)

- **예시 k=5**
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5aac6fb1-31bb-445f-860f-75f66cd1bbc5/Untitled.png)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0ac196f3-9f9a-4858-8cd2-8986facd5686/Untitled.png)
    

## local reachability density

$lrd_k(A) = \cfrac{|N_k(p)|}{\sum_{O \in N_k(A)} reachability-distance_k(A, B)}$

: A로부터 다른 오브젝트들까지의 reachability distance들의 평균 식의 역수

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/776ba1fa-c8cf-4eb2-8f5e-07a82cd9e646/Untitled.png)

- Case 1 : A가 밀도가 높은 지역에 위치, 분모는 작아지며 $lrd_k(A)$ 값이 커짐
- Case 2 : A가 밀도가 낮은 지역에 위치, 분모는 커지며 $lrd_k(A)$ 값이 작아짐

## Local Outlier Factor

$LOF_K(A) = \cfrac{\sum_{B \in N_K(A) }\frac{lrd_k(B)}{lrd_k(A)}}{|N_k(A)|} = \cfrac {\frac{1}{lrd_k(A)} \sum_{B \in N_k(A)} lrd_k(B)}{|N_k(A)|}$

- LOF < 1 : 밀도가 높은 분포
- LOF ≒ 1 : 이웃 관측치와 비슷한 분포
- LOF > 1 : 밀도가 낮은 분포, 크면 클수록 이상치 정도가 큼.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/31a3139a-4c8f-49c8-b38a-4084e696e16e/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8dee1b8f-5f1d-4b73-8b86-343b5528afd7/Untitled.png)

⇒ 파란점(중심) 밀도가 낮은 지역일수록, 초록점(주변) 밀도가 높은 지역일수록 파란점의 LOF 값이 커짐

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/53ae9714-4676-44b2-a5b0-7409aaef1dc0/Untitled.png)

## 코드

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')

np.random.seed(42)

# Generate train data
X_inliers = 0.3 * np.random.randn(100, 2)#정규분포에서 100*2만들고
X_inliers = np.r_[X_inliers + 2, 2*X_inliers - 2]#각각 2,2 혹은 -2,-2만큼 평행이동한거를 vstack. 즉 cluster 2개

# Generate some outliers
X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
X = np.r_[X_inliers, X_outliers]#-4,4에서 뽑은 outlier와 inlier를 vstack

n_outliers = len(X_outliers)
ground_truth = np.ones(len(X), dtype=int)
ground_truth[-n_outliers:] = -1

# fit the model for outlier detection (default)
clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
# use fit_predict to compute the predicted labels of the training samples
# (when LOF is used for outlier detection, the estimator has no predict,
# decision_function and score_samples methods).
y_pred = clf.fit_predict(X) #1,-1로 나온다.
n_errors = (y_pred != ground_truth).sum()
X_scores = clf.negative_outlier_factor_

#fig, ax = plt.subplots()
plt.title("Local Outlier Factor (LOF)")
plt.scatter(X[:, 0], X[:, 1], color='b', s=3., label='Data points')
# plot circles with radius proportional to the outlier scores
radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min()) #오홍 minmax scaling으로 radius를 정햇네
plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',
            facecolors='none', label='Outlier scores')
n=np.copy(X_scores)
n[n>-1.3]=np.nan
n=np.round(n,2)
for i, txt in enumerate(n):
    if np.isnan(txt):continue
    plt.annotate(txt, (X[i,0], X[i,1]))
legend = plt.legend(loc='upper left')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/626ab035-5843-4900-9b0e-0ad9d1ffeb43/Untitled.png)

## LOF 이슈

- K값 지정: 2~6
- LOF가 어떤 값보다 커야 이상치로 정의? (thresholding)
- LOF값의 이상치 여부는 데이터 셋마다 상대적 → 다른 데이터셋에서 동일한 LOF값이어도 이상치 여부는 다름
- 계산복잡도가 큼
- 고차원일 경우, 착시현상이 존재→일반적인 거리 측정이 직관과 다른 경우가 존재(먼 경우가 가깝게, 가까운 경우가 멀게)

## **참고자료**

[sklearn.neighbors.LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor.predict)

[[Code implementation] Gaussian mixture · Go's BLOG](https://godongyoung.github.io/머신러닝/2019/10/15/Gaussian-Mixture-EM-algorithm.html)

[[핵심 머신러닝 ] Anomaly Detection - Local Outlier Factor (LOF)](https://www.youtube.com/watch?v=wADcqMdpuv4)

---

# DAGMM (Deep Autoencoder Gaussian Mixture Model)

## GMM(Gaussian Mixture Model)

가우시안 분포가 혼합된 모델로 클러스터링 알고리즘

정해진 k에 따라 복잡한 형태의 분포를 k개의 가우시안 분포로 쪼갬

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b6cab6c7-e8be-4d18-841d-6bbb8276d790/Untitled.png)

$**parameter(\pi, \mu, \Sigma)**$

- $\pi_k$ : 각 관찰치가 k번째 군집에 포함될(선택될) 확률
- $\mu_k$ : k번째 군집의 평균
- $\Sigma_k$ : 군집의 분산

**x가 발생할 확률** → 가우시안 확률 밀도 함수의 합

$p(x) = \sum_{k=1}^{K}\pi_kN(x|\mu_k, \Sigma_k)$

### GMM을 이용한 classification

주어진 데이터 $x_n$에 대해 어떤 가우시안 분포에서 생성되었는지 찾는 문제

$\gamma(z_{nk}) = p(z_{nk}=1|x_n)$

- $z_{nk} \in \{0, 1\}$ : $x_n$이 주어졌을 때, GMM의 k번째 가우시안 분포가 선택되면 1, 아니면 0의 값을 갖는 이진 변수

$x_n$이 주어졌을 때, k개의 $\gamma(z_{nk})$를 계산해 가장 높은 가우시안 분포를 선택

$\gamma(z_{nk}) = p(z_{nk}=1|x_n) = \cfrac{p(z_{nk}=1)p(x_n|z_{nk}=1)}{\sum_{j=1}^{K}p(z_{nj}=1)p(x_n|z_{nj}=1)} = \cfrac{\pi_kN(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_jN(x_n|\mu_j, \Sigma_j)}$

### EM algorithm(Expectation-maximization algorithm)

EM 알고리즘을 적용해 GMM의 파라미터 $\pi, \mu, \Sigma$를 추정

**Log-likelihood $L(X;\theta)$ 정의**

$L(X;\theta) = \ln p(X|\pi, \mu, \Sigma) = \ln \{\Pi_{n=1}^{N}p(x_n|\pi, \mu, \Sigma) \} = \sum_{n=1}^{N}\ln\{\sum_{k=1}^{K}\pi_kN(x_n|\mu_k, \Sigma_k)\}$

- $\mu_k$ 추정 : 로그가능도 함수를 최대화
    
    → 로그가능도 편미분 $\frac{\partial L(X|\theta)} {\partial \mu_k} = 0$ → $\mu_k$ 추정
    
- $\Sigma_k$ 추정 : 주어진 $\hat\mu_k$에서, 로그가능도 함수를 최대화
    
    → 로그가능도 편미분 $\frac{\partial L(X|\theta)} {\partial \Sigma_k} = 0$ → $\Sigma_k$ 추정
    
- $\pi_k$ 추정 : $\sum_{k=1}^{K}\pi_k =1$ 조건에서, 로그가능도 함수를 최대화
    
    → 라그랑주 승수법 
    
    1. $J(X;\theta, \lambda) = \sum_{n=1}^{N}\ln\{\sum_{k=1}^{K}\pi_kN(x_n|\mu_k, \Sigma_k)\} + \lambda(1-\sum_{k=1}^{K}\pi_k)$
    2. 라그랑지안 편미분 $\frac{\partial J(X;\theta, \lambda)}{\partial\pi_k}$ → $\lambda$  추정 
    3. 계산한 $\lambda$를 이용해 라그랑지안 편미분 $\frac{\partial J(X;\theta, \lambda)}{\partial\pi_k}$ → $\pi_k$ 추정

1. **E-step**
    
    모든 데이터와 가우시안 분포에 대해 $\gamma(z_{nk})$ 계산
    
2. **M-step**
    
    모든 가우시안 분포에 대해 $\pi, \mu, \Sigma$ 추정
    

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8b03b1f2-cec4-4db9-9584-8d10201bacd4/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2c1779b4-0908-4c3b-a072-28daa274b403/Untitled.png)

---

## 특징

1. 오토인코더 사용 → 축소된 차원과 복원 오차에 대한 특성 유지 ⇒ 입력값의 중요 정보 저차원상에서 유지 가능
2. 학습된 저차원 공간에서 GMM 활용해 복잡한 구조의 입력 데이터에 대한 밀도 추정 가능
3. end-to-end 학습에 적합(local-optima 지역 최적값 문제 없음)

## 구조

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ff5da02-a30a-4eee-8f4d-714b94056c2e/Untitled.png)

- 압축 네트워크(auto encoder)
- 추정 네트워크(GMM)

### 압축 네트워크

deep auto encoder 사용해 입력 데이터 차원 축소

- $Z_c$ : 차원 축소를 통해 축소된 저차원 데이터
    - $z_c = h(x;\theta_e)$    ; $h$: 인코더 함수
- $Z_r$ : 원 데이터와 복원 데이터의 오차(재구축 오차)에서 파생된 변수 (여러 차원 가능)
    - $z_r = f(x, x')$    ; $f$: 재구축 오차 계산 함수
        - $x' = g(z_c;\theta_d)$    ; $g$: 디코더 함수
- $z = [z_c, z_r]$

### 추정 네트워크

차원 축소된 데이터를 입력값으로, GMM으로 밀도 추정

추정된 파라미터를 사용해 sample energy를 계산

기준값보다 큰 energy를 갖는 데이터를 비정상 자료로 분류

**EM 알고리즘**

1. **E-step :** 혼합 구성요소의 수 $K$, 저차원 표현 $\mathbf z$로 $\hat \gamma$ 추정
    
    $\mathbf p = MLN(\mathbf z;\theta_m)$        ; $p$는 $\theta_m$에 의한 다층 네트워크의 결과
    
    $\hat \gamma = softmax(\mathbf p)$         
    

1. **M-step :** GMM 파라미터 추정
    
    $\hat \phi_k = \sum_{i=1}^{N}\cfrac{\hat \gamma_{ik}}{N}$
    
    $\hat \mu_k = \cfrac{\sum_{i=1}^{N}\hat \gamma_{ik}\mathbf z_i}{\sum_{i=1}^{N}\hat \gamma_{ik}}$
    
    $\hat\Sigma_k = \cfrac{\sum_{i=1}^{N}\hat\gamma_{ik}(\mathbf z_i - \hat \mu_k){(\mathbf z_i - \hat \mu_k)^T}}{\sum_{i=1}^{N}\hat\gamma_{ik}}$
    

**sample energy**

$E(\mathbf z) = -\log(\sum_{k=1}^{K}\hat{\phi}_k\cfrac{exp(-{1\over2}(\mathbf z - \hat \mu_k))^T\hat \Sigma_k^{-1}(\mathbf z - \hat \mu_k))}{\sqrt{|2\pi\hat\Sigma_k|}})$

### 목적함수

$Jc(\theta_e, \theta_d, \theta_m) = \cfrac{1}{N} \sum_{i=1}^{N}L(\mathbf x_i, \mathbf x_i') + \cfrac{\lambda_1}{N}\sum_{i=1}^{N}E(\mathbf z_i) + \lambda_2P(\hat{\mathbf\Sigma})$

- $L(\mathbf x_i, \mathbf x_i')$ : 압축 네트워크의 복원 오차(재구축) → 최소화(L2-norm)
- $E(\mathbf z_i)$ : 입력 샘플을 관찰할 수 있는 확률을 모델링 sample energy → 최소화
    
    ⇒ 입력 샘플 likelihood 가능도 최대화
    
- $P(\hat \Sigma) = \sum_{k=1}^K \sum_{j=1}^{d} {1 \over \hat \Sigma_{kij}}$ : 공분산 행렬에 벌점화 함수 추가
    
    $\because$  특이성 문제 : 공분산 행렬의 대각원소가 0으로 퇴보
    

## **참고자료**

[DAGMM/KDDCup99.ipynb at master · tnakae/DAGMM](https://github.com/tnakae/DAGMM/blob/master/KDDCup99.ipynb)

[https://github.com/hoya012/awesome-anomaly-detection](https://github.com/hoya012/awesome-anomaly-detection)

**논문**

[https://bzong.github.io/doc/iclr18-dagmm.pdf](https://bzong.github.io/doc/iclr18-dagmm.pdf)
